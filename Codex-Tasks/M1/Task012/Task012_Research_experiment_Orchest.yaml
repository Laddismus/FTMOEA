title: AFTS_M1_Task_012_Research_Experiment_Orchestrator_v1

summary: |
  Baue einen Experiment-Orchestrator im Research Backend, der mehrere Backtests
  als ein strukturiertes "Experiment" verwaltet:
    - Ein Experiment beschreibt: Strategie-Ref, Parameter-Varianten (Grid), Kosten- & FTMO-Config,
      sowie optionale Metadaten (Dataset-ID, Notizen).
    - Der Orchestrator erstellt daraus eine Menge von BacktestRequests,
      feuert sie über den bestehenden BacktestService/JobRunner ab und verwaltet deren Status.
    - Ergebnisse (BacktestResults inkl. KPIs, FTMO-Summary) werden dem Experiment zugeordnet
      und können per API abgefragt werden (Experiment-Übersicht + Runs).

  Ziel v1:
    - Reine Orchestrierungsschicht, kein UI.
    - Speichert Experimente (inkl. Backtest-IDs) als JSON-Artefakte.
    - API: Experiment anlegen, Status holen, Runs und KPIs einsehen.
    - Vollständig getestet mit pytest.

inputs:
  - Aktueller Stand Research Backend:
      - Backtests:
          - research_lab/backend/core/backtests/models.py
          - research_lab/backend/core/backtests/service.py
          - research_lab/backend/core/backtests/persistence.py
          - research_lab/backend/app/api/backtests.py
      - Strategy Execution & Registry:
          - research_lab/backend/core/strategy_execution/* (graph/python Executor)
          - research_lab/backend/core/python_strategies/* (Interface, Loader, Registry)
          - research_lab/backend/core/strategy_builder/* (für spätere Graph-Experimente)
      - Risk & Costs:
          - research_lab/backend/core/risk_guard/* (FtmoRiskGuard, Config, Summary)
          - BacktestCostModel in BacktestRequest
      - Infrastructure:
          - InMemoryJobRunner (job_runner.py)
          - ResearchSettings (settings.py) inkl. backtests_dir etc.
      - Tests:
          - Bestehende Tests zu Backtests, FTMO-Risk, Persistence, API.

outputs:
  - Experiment-Domain-Modelle:
      - Neues Paket: research_lab/backend/core/experiments/__init__.py
      - research_lab/backend/core/experiments/models.py

        ```python
        from datetime import datetime
        from enum import Enum
        from typing import Any, Dict, List, Optional
        from pydantic import BaseModel

        class ExperimentStatus(str, Enum):
            PENDING = "pending"
            RUNNING = "running"
            COMPLETED = "completed"
            FAILED = "failed"

        class ExperimentStrategyRef(BaseModel):
            mode: str  # "python" oder "graph"
            python_strategy: Optional[PythonStrategyRef] = None
            graph: Optional[StrategyGraphRef] = None

        class ExperimentParamPoint(BaseModel):
            """
            Konkreter Parameterpunkt eines Grids, z. B. {"size": 1.0, "fee_rate": 0.0004}
            """
            values: Dict[str, Any]

        class ExperimentConfig(BaseModel):
            id: str
            name: str
            description: str | None = None
            created_at: datetime
            strategy: ExperimentStrategyRef
            base_backtest: BacktestRequest
            param_grid: List[ExperimentParamPoint]
            # Metadaten:
            tags: List[str] = []
            metadata: Dict[str, Any] = {}

        class ExperimentRunStatus(BaseModel):
            run_id: str
            backtest_id: Optional[str] = None   # ID des BacktestResult in Persistence
            job_id: Optional[str] = None        # JobRunner-ID
            status: ExperimentStatus
            error: Optional[str] = None

        class ExperimentSummary(BaseModel):
            id: str
            name: str
            status: ExperimentStatus
            created_at: datetime
            total_runs: int
            completed_runs: int
            failed_runs: int
            # Optional: einfache Aggregats-KPIs (z. B. bester PF, med PF)
            best_total_return: Optional[float] = None
            best_pf: Optional[float] = None
        ```

        - Hinweis:
          - PythonStrategyRef und StrategyGraphRef stammen aus backtests/models.py bzw. python_strategies/strategy_builder
            – du kannst sie hier importieren oder spiegeln; wichtig ist Konsistenz.

  - Experiment-Persistenz:
      - research_lab/backend/core/experiments/persistence.py

        ```python
        class ExperimentPersistence:
            def __init__(self, experiments_dir: Path):
                self.experiments_dir = experiments_dir
                self.experiments_dir.mkdir(parents=True, exist_ok=True)

            def save_experiment(self, config: ExperimentConfig, runs: List[ExperimentRunStatus]) -> None:
                ...

            def load_experiment(self, experiment_id: str) -> tuple[ExperimentConfig, List[ExperimentRunStatus]] | None:
                ...

            def list_experiments(self) -> list[ExperimentSummary]:
                ...
        ```

        - Storage-Modell v1:
          - Pro Experiment eine JSON-Datei: `<experiments_dir>/<experiment_id>.json`, mit Feldern:
              - "config": ExperimentConfig.dict()
              - "runs": [ExperimentRunStatus.dict(), ...]
          - list_experiments():
              - liest alle JSONs, leitet status (Pending/Running/Completed/Failed) aus runs ab:
                  - Completed: alle runs status COMPLETED,
                  - Failed: mind. ein run FAILED, Rest Completed,
                  - Running: mind. ein run RUNNING,
                  - Pending: noch keine Jobs gestartet oder alle PENDING.
              - berechnet summary-Felder:
                  - total_runs
                  - completed_runs
                  - failed_runs
                  - optional best_total_return / best_pf:
                      - liest dazu BacktestResults über BacktestPersistence, falls du das in diesem Task schon einbauen willst;
                        alternativ kannst du best_* Felder vorerst None lassen.

  - ExperimentService:
      - research_lab/backend/core/experiments/service.py

        ```python
        class ExperimentService:
            def __init__(
                self,
                backtest_service: BacktestService,
                experiment_persistence: ExperimentPersistence,
            ):
                self.backtest_service = backtest_service
                self.experiment_persistence = experiment_persistence

            def create_experiment(self, config: ExperimentConfig) -> ExperimentConfig:
                # initial PENDING runs, noch keine Jobs gestartet
                ...

            def launch_experiment(self, experiment_id: str) -> None:
                """
                Erzeugt für jedes ParamPoint einen BacktestRequest, startet Jobs via BacktestService.submit_job
                und aktualisiert die Run-Status.
                """
                ...

            def refresh_status(self, experiment_id: str) -> tuple[ExperimentConfig, list[ExperimentRunStatus]]:
                """
                Holt für alle Runs den aktuellen Job-Status (über JobRunner/BacktestService)
                und aktualisiert das Experiment (persistiert).
                """
                ...

            def get_experiment(self, experiment_id: str) -> tuple[ExperimentConfig, list[ExperimentRunStatus]] | None:
                ...

            def list_experiments(self) -> list[ExperimentSummary]:
                ...
        ```

        - Backtest-Erzeugung:
          - Für jeden ExperimentParamPoint:
              - Kopie von base_backtest (Deepcopy).
              - Merge von `param_point.values` in `base_backtest.strategy_params`, `base_backtest.cost_model`,
                `base_backtest.ftmo_risk` etc., je nach Key-Konvention:
                  - z. B. Keys mit Prefix "cost_model." gehen ins cost_model-Feld,
                    "ftmo_risk." ins FTMO-Config-Feld, Rest in strategy_params.
                  - Für v1 kannst du es simpel halten:
                      - param_point.values → strategy_params-Update.
          - Für jeden BacktestRequest:
              - `job_id = backtest_service.submit_job(request)`
              - run_status = ExperimentRunStatus(run_id=uuid4, job_id=job_id, status=RUNNING)
          - Nach launch_experiment:
              - ExperimentConfig + runs speichern.

        - Status-Refresh:
          - Für jeden Run mit job_id:
              - `job_info = backtest_service.get_job_result(job_id)`
              - Falls job_info.result vorhanden:
                  - setze run_status.status = COMPLETED.
                  - hole `backtest_id` aus result.id.
              - Falls Error-Feld o. ä. im Job vorhanden (InMemoryRunner ggf. erweitern):
                  - status = FAILED, error=...
          - Persistiere aktualisiertes Experiment.

  - Settings:
      - In research_lab/backend/settings.py:
          - Neues Verzeichnis für Experimente:

            ```python
            class ResearchSettings(...):
                ...
                @property
                def experiments_dir(self) -> Path:
                    return self.artifacts_dir / "research" / "experiments"
            ```

          - Experiments-Verzeichnis im Repo ggf. mit .gitkeep scaffolden (artifacts/research/experiments/.gitkeep).

  - API:
      - Neue Datei: research_lab/backend/app/api/experiments.py

        ```python
        router = APIRouter(prefix="/experiments", tags=["experiments"])
        ```

        - Endpoints v1:
          1. `POST /api/experiments`
              - Body: ExperimentCreateRequest (angepasste API-Variante von ExperimentConfig, ohne id/created_at).
              - Server generiert:
                  - experiment_id (uuid4),
                  - created_at (UTC),
              - Legt Experiment mit status PENDING an.
              - Response: ExperimentConfig (inkl. id).

          2. `POST /api/experiments/{experiment_id}/launch`
              - Effekt:
                  - Ruft ExperimentService.launch_experiment(experiment_id) auf.
              - Response: ExperimentSummary nach Launch (optional) oder 204/202.

          3. `GET /api/experiments`
              - Response: list[ExperimentSummary]
              - Nutzt ExperimentService.list_experiments().

          4. `GET /api/experiments/{experiment_id}`
              - Response:
                  - ExperimentConfig + Runs (z. B. als ExperimentDetailResponse mit config + runs).
              - Nutzt ExperimentService.get_experiment() + ggf. refresh_status().

          5. Optional v1+: `POST /api/experiments/{experiment_id}/refresh`
              - Ruft refresh_status(experiment_id) auf und gibt aktualisiertes Detail zurück.

        - In research_lab/backend/app/main.py:
            - `from research_lab.backend.app.api import experiments`
            - `app.include_router(experiments.router, prefix="/api")`

  - Tests (pytest) – Experiment Core & API:
      - tests/research_lab/backend/test_experiment_persistence.py
          - Nutzt tmpdir als experiments_dir + einen minimalen BacktestRequest.
          - Test 1:
              - ExperimentConfig + Runs erzeugen (1–2 Runs).
              - save_experiment() → JSON-Datei existiert.
              - load_experiment() → identische Config + Runs.
          - Test 2:
              - Zwei Experimente speichern.
              - list_experiments() → 2 Summaries mit korrekten counts (total_runs, completed_runs etc.).
              - Status-Logik (PENDING/RUNNING/COMPLETED/FAILED) anhand von Runs verifizieren.

      - tests/research_lab/backend/test_experiment_service.py
          - Mock/Stub BacktestService (oder kleinaufgesetzt mit InMemoryJobRunner & RollingKpiBacktestEngine).
          - Test 1 (create_experiment):
              - ExperimentConfig mit param_grid (2 Punkte) anlegen.
              - create_experiment() → Persistenz aufgerufen, Status PENDING.
          - Test 2 (launch_experiment):
              - Launch ruft für jeden ParamPoint submit_job() auf.
              - Runs haben status RUNNING, job_id gesetzt.
          - Test 3 (refresh_status):
              - Simuliere fertige Jobs (InMemoryJobRunner result gesetzt).
              - refresh_status() → RUNNING → COMPLETED, backtest_id gesetzt.

      - tests/research_lab/backend/test_experiments_api.py
          - FastAPI TestClient.
          - Setup:
              - ExperimentService mit BacktestService (InMemory) & ExperimentPersistence(tmpdir).
          - Testfälle:
              - `POST /api/experiments`:
                  - Erzeugt Experiment mit name, strategy, base_backtest, param_grid.
                  - Response 200, enthält experiment_id.
              - `POST /api/experiments/{id}/launch`:
                  - Status 200/202/204 (wie du definierst).
              - `GET /api/experiments`:
                  - Liste ≥ 1 ExperimentSummary.
              - `GET /api/experiments/{id}`:
                  - Gibt config + runs zurück, wobei runs status != PENDING nach einem Refresh
                    (du kannst optional im GET automatisch refresh_status aufrufen).

acceptance: |
  Dieser Task ist DONE, wenn:

  1. Domain:
     - Experiment-Domain-Modelle (ExperimentConfig, ExperimentParamPoint, ExperimentRunStatus, ExperimentSummary)
       sind implementiert und typisiert.
     - ExperimentStatus-Enum spiegelt PENDING/RUNNING/COMPLETED/FAILED wider.

  2. Persistenz:
     - ExperimentPersistence speichert und lädt Experimente inkl. Runs als JSON.
     - list_experiments() liefert eine korrekte Liste von ExperimentSummary (Status, Counts).

  3. Service:
     - ExperimentService:
         - create_experiment() legt ein Experiment mit PENDING-Runs an.
         - launch_experiment() erstellt pro ParamPoint einen BacktestRequest und startet Jobs via BacktestService.
         - refresh_status() aktualisiert Run-Status basierend auf JobRunner/BacktestService.
         - get_experiment() und list_experiments() liefern konsistente Daten.

  4. API:
     - /api/experiments (POST, GET), /api/experiments/{id} (GET) und /api/experiments/{id}/launch (POST)
       sind implementiert und in main.py registriert.
     - API-Tests zeigen, dass:
         - Experimente angelegt werden können.
         - Launch Jobs startet.
         - Experimente mit Runs abrufbar sind.

  5. Tests:
     - test_experiment_persistence.py, test_experiment_service.py, test_experiments_api.py
       sind implementiert und grün.
     - Alle bestehenden Tests bleiben grün (ggf. minimale Anpassungen für Settings/Imports).
     - Gesamt-`pytest` im Projekt-Root (afts_pro) läuft ohne Errors/Failures.

  6. Qualität:
     - Vollständige Type Hints und Docstrings für:
         - ExperimentPersistence,
         - ExperimentService,
         - zentrale Pydantic-Models.
     - Design ist so, dass in späteren Tasks:
         - komplexere Param-Grids (z. B. kartesische Produkte),
         - Auswertung/Ranking (Top-N-Strategien nach PF, FTMO-Pass-Quote),
         - sowie UI/Reports
       ohne Breaking Changes auf diesem Orchestrator aufsetzen können.

coding_standards: |
  - Python >= 3.11.
  - Typisierung überall, BacktestService/ExperimentService bleiben getrennt (Single Responsibility).
  - Router dünn halten; Orchestrierungslogik nur im Core (experiments/*).
  - Tests deterministisch, tmpdir/monkeypatch für experiments_dir & Settings.

notes: |
  - Mit diesem Task hebst du den Research-Stack von "Einzel-Backtest" auf "Experiment-Level":
      - Du kannst parametrisierte Sweeps für Strategien fahren,
      - FTMO-/Costs-/Strategy-Param-Varianten als ein Experiment betrachten,
      - und später im UI eine Experiment-Tabelle / Ranking anzeigen.
  - Spätere M1.x Tasks können:
      - automatische KPI-Aggregate in ExperimentSummary bauen,
      - eine kleine "Leaderboard"-API anbieten,
      - und das Ganze mit deiner Analytics-Suite (FeatureExplorer, KPIEngine, Clustering)
        verknüpfen.
